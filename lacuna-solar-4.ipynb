{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10783793,"sourceType":"datasetVersion","datasetId":6691299}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the timm library\n# !pip install timm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchvision import transforms\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom PIL import Image\nimport timm  # Import timm library\n\n# Define Transforms for Data Augmentation and Preprocessing\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to a fixed size\n    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n    transforms.RandomVerticalFlip(),  # Randomly flip images vertically\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to a fixed size\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n])\n\n# Custom Dataset Class\nclass SolarPanelDataset(Dataset):\n    def __init__(self, df, transform=None, to_train=True):\n        self.df = df\n        self.transform = transform\n        self.to_train = to_train\n        self.image_dir = \"/kaggle/input/lacuna-solar-survey-challenge/images\"\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.df.iloc[idx][\"ID\"] + \".jpg\")\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # Process metadata\n        metadata = self.df.iloc[idx][[\"img_origin\", \"placement\"]]\n        img_origin = 0 if metadata[\"img_origin\"] == \"S\" else 1  # S -> 0, D -> 1\n        placement = {\"roof\": 0, \"openspace\": 1, \"r_openspace\": 2, \"S-unknown\": 3}[metadata[\"placement\"]]\n        metadata_encoded = img_origin * 10 + placement  # Combine into a single integer\n\n        if self.to_train:\n            target = self.df.iloc[idx][[\"pan_nbr\", \"boil_nbr\"]].values.astype(np.float32)\n            return image, metadata_encoded, target\n        else:\n            return image, metadata_encoded\n\n# Advanced Model with ConvNeXt Backbone from timm\nclass AdvancedModel(nn.Module):\n    def __init__(self):\n        super(AdvancedModel, self).__init__()\n        self.backbone = timm.create_model(\"convnext_base\", pretrained=True, num_classes=0)  # Load ConvNeXt from timm\n        self.metadata_embedding = nn.Embedding(num_embeddings=14, embedding_dim=128)  # 14 possible metadata values (0-13)\n        self.fc = nn.Linear(1024 + 128, 2)  # Combine image and metadata features\n\n    def forward(self, images, metadata):\n        image_features = self.backbone(images)\n        metadata_features = self.metadata_embedding(metadata)\n        combined_features = torch.cat([image_features, metadata_features], dim=1)\n        return self.fc(combined_features)\n\n# Training Function\ndef train(fold, train_loader, val_loader, epochs, batch_size):\n    model = AdvancedModel().cuda()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n    scaler = GradScaler()  # Correct initialization for older PyTorch versions\n    criterion = nn.L1Loss()  # MAE for counting\n\n    best_mae = float(\"inf\")\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for images, metadata, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            images, metadata, targets = images.cuda(), metadata.cuda(), targets.cuda()\n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(images, metadata)\n                loss = criterion(outputs, targets)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            train_loss += loss.item()\n        scheduler.step()\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for images, metadata, targets in val_loader:\n                images, metadata, targets = images.cuda(), metadata.cuda(), targets.cuda()\n                outputs = model(images, metadata)\n                val_loss += criterion(outputs, targets).item()\n        val_loss /= len(val_loader)\n        print(f\"Fold {fold+1}, Epoch {epoch+1}: Train Loss = {train_loss/len(train_loader):.4f}, Val Loss = {val_loss:.4f}\")\n\n        if val_loss < best_mae:\n            best_mae = val_loss\n            torch.save(model.state_dict(), f\"best_model_fold{fold}.pth\")\n    return best_mae\n\n# Inference Function\ndef predict(test_df, model_paths, batch_size):\n    test_ds = SolarPanelDataset(test_df, transform=val_transform, to_train=False)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    predictions = np.zeros((len(test_df), 2))\n    for path in model_paths:\n        model = AdvancedModel().cuda()\n        model.load_state_dict(torch.load(path))\n        model.eval()\n\n        with torch.no_grad():\n            for images, metadata in tqdm(test_loader, desc=\"Inference\"):\n                images, metadata = images.cuda(), metadata.cuda()\n                outputs = model(images, metadata)\n                predictions += outputs.cpu().numpy()\n    return predictions / len(model_paths)\n\n# Main Execution\nif __name__ == \"__main__\":\n    # Load data\n    train_df = pd.read_csv(\"/kaggle/input/lacuna-solar-survey-challenge/Train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/lacuna-solar-survey-challenge/Test.csv\")\n\n    # Cross-validation\n    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n    model_paths = []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df[\"placement\"])):\n        print(f\"Training fold {fold+1}\")\n        train_dataset = SolarPanelDataset(train_df.iloc[train_idx], transform=train_transform)\n        val_dataset = SolarPanelDataset(train_df.iloc[val_idx], transform=val_transform)\n        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n        best_mae = train(fold, train_loader, val_loader, epochs=25, batch_size=16)\n        model_paths.append(f\"best_model_fold{fold}.pth\")\n\n    # Inference\n    predictions = predict(test_df, model_paths, batch_size=64)\n\n    # Create submission\n    submission = pd.DataFrame({\n        \"ID\": np.repeat(test_df[\"ID\"].values, 2),\n        \"Target\": predictions.flatten()\n    })\n    submission[\"ID\"] += np.where(\n        submission.groupby(\"ID\").cumcount() == 0,\n        \"_boil\",\n        \"_pan\"\n    )\n    submission.to_csv(\"submission_original.csv\", index=False)\n\n    # Integer submission\n    int_submission = submission.copy()\n    int_submission[\"Target\"] = np.round(int_submission[\"Target\"]).astype(int)\n    int_submission.to_csv(\"submission_integer.csv\", index=False)\n\n    print(\"Submissions saved with shapes:\", submission.shape, int_submission.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T13:42:33.293698Z","iopub.execute_input":"2025-03-18T13:42:33.294123Z","execution_failed":"2025-03-18T17:00:48.944Z"}},"outputs":[{"name":"stdout","text":"Training fold 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c43447e0c94dd58209829def56bde8"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-1-90aeefbcaff1>:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()  # Correct initialization for older PyTorch versions\nEpoch 1/25:   0%|          | 0/283 [00:00<?, ?it/s]<ipython-input-1-90aeefbcaff1>:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1/25: 100%|██████████| 283/283 [06:47<00:00,  1.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 1: Train Loss = 1.0573, Val Loss = 0.9105\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25: 100%|██████████| 283/283 [06:29<00:00,  1.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 2: Train Loss = 0.9125, Val Loss = 0.8293\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25: 100%|██████████| 283/283 [06:29<00:00,  1.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 3: Train Loss = 0.8317, Val Loss = 0.8175\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25: 100%|██████████| 283/283 [06:27<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 4: Train Loss = 0.7898, Val Loss = 0.7798\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25: 100%|██████████| 283/283 [06:27<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 5: Train Loss = 0.7361, Val Loss = 0.7635\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25: 100%|██████████| 283/283 [06:26<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 6: Train Loss = 0.7188, Val Loss = 0.7703\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25: 100%|██████████| 283/283 [06:29<00:00,  1.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 7: Train Loss = 0.7031, Val Loss = 0.7389\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25: 100%|██████████| 283/283 [06:26<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 8: Train Loss = 0.6540, Val Loss = 0.7257\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25: 100%|██████████| 283/283 [06:31<00:00,  1.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 9: Train Loss = 0.6345, Val Loss = 0.7380\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25: 100%|██████████| 283/283 [06:26<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 10: Train Loss = 0.6244, Val Loss = 0.7387\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25: 100%|██████████| 283/283 [06:25<00:00,  1.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 11: Train Loss = 0.6023, Val Loss = 0.7279\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25: 100%|██████████| 283/283 [06:28<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 12: Train Loss = 0.5930, Val Loss = 0.7053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25: 100%|██████████| 283/283 [06:27<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Epoch 13: Train Loss = 0.5702, Val Loss = 0.7164\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/25:   6%|▌         | 17/283 [00:27<08:27,  1.91s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}